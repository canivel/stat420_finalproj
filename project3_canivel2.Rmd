---
title: "project"
author: "Danilo Canivel(canivel2)"
date: "7/31/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(lubridate)
library(ggplot2)
library(cluster)
library(tidyverse)
library(caret)
library(magrittr)
library(Matrix)
library(faraway)
```


## Read Data

** We are using a small chunk of the data just to be able to run some local hipotheses**

```{r}
nrows = 100000
fdata = read.csv("./input/train.csv", stringsAsFactors=FALSE, nrows=nrows)
```

## Feature Engineering

** Several features are been created on this step: **
  - First if we are processing for training, we are gonna remove several outliers relate to fare amount, location and passenger count
  - Next we fatorize some of the features to work as categories
  - time for example is factorize in 6 different categories
  - the locations are converted in radius 
  - We decided to add a few locations where taxis usually take or drop passengers in NYC.
  - We Compute haversine distance and Bearing distances
    - https://en.wikipedia.org/wiki/Haversine_formula
    - https://en.wikipedia.org/wiki/Bearing_(navigation)

```{r}
earth_radius = 6371 #in kms

sphere_dist = function(pickup_lat, pickup_lon, dropoff_lat, dropoff_lon)
{
    #Compute distances along lat, lon dimensions
    dlat = dropoff_lat - pickup_lat
    dlon = dropoff_lon - pickup_lon
    
    #Compute  distance
    a = sin(dlat/2.0)**2 + cos(pickup_lat) * cos(dropoff_lat) * sin(dlon/2.0)**2
    
    return (2 * earth_radius * asin(sqrt(a)))
    
} 

features_eng = function(data, istrain=TRUE){
  
  if(istrain == TRUE){
    data = data[complete.cases(data),]
    # Remove Negative Fare and No Passenger and some places that does not exist
    data = data[(data$fare_amount > 0 & data$fare_amount <= 500),]
    data = data[(data$pickup_longitude > -80 && data$pickup_longitude < -70),]
    data = data[(data$pickup_latitude > 35 && data$pickup_latitude < 45),]
    data = data[(data$dropoff_longitude > -80 && data$dropoff_longitude < -70),]
    data = data[(data$dropoff_latitude > 35 && data$dropoff_latitude < 45),]
    #only if passengers are between 1 and 9
    data = data[(data$passenger_count > 0 && data$passenger_count < 10),]
  }
  
  # Convert to datetime obj
  data$pickup_datetime = as_datetime(data$pickup_datetime, tz="UTC")

  # factorization
  data$year = as.factor(year(data$pickup_datetime))
  data$month = as.factor(month(data$pickup_datetime))
  data$day = as.factor(day(data$pickup_datetime))
  data$weekday = as.factor(weekdays(data$pickup_datetime))

  #factor time of the day
  hour = hour(data$pickup_datetime)
  data$time = as.factor(ifelse(hour < 7, "Overnight", 
                               ifelse(hour < 11, "Morning", 
                                      ifelse(hour < 16, "Noon",
                                             ifelse(hour < 20, "Evening",
                                                    ifelse(hour < 23, "night", "overnight")
                        )))))
  # convert to radius position
  data$pickup_latitude = (data$pickup_latitude * pi)/180
  data$dropoff_latitude = (data$dropoff_latitude * pi)/180
  data$dropoff_longitude = (data$dropoff_longitude * pi)/180
  data$pickup_longitude = (data$pickup_longitude * pi)/180 
  data$dropoff_longitude = ifelse(is.na(data$dropoff_longitude) == TRUE, 0,data$dropoff_longitude)
  data$pickup_longitude = ifelse(is.na(data$pickup_longitude) == TRUE, 0,data$pickup_longitude)
  data$pickup_latitude = ifelse(is.na(data$pickup_latitude) == TRUE, 0,data$pickup_latitude)
  data$dropoff_latitude = ifelse(is.na(data$dropoff_latitude) == TRUE, 0,data$dropoff_latitude)

  # compare locations
  data$dlat = data$dropoff_latitude - data$pickup_latitude
  data$dlon = data$dropoff_longitude - data$pickup_longitude 
  
  #Compute haversine distance
  
  data$hav = sin(data$dlat/2.0)**2 + cos(data$pickup_latitude) * cos(data$dropoff_latitude) * sin(data$dlon/2.0)**2
  data$haversine = round(x = 2 * earth_radius * asin(sqrt(data$hav)), digits = 4)
  print(dim(data))
  
  #Compute Bearing distance
  data$dlon = data$pickup_longitude - data$dropoff_longitude
  data$bearing = atan2(sin(data$dlon * cos(data$dropoff_latitude)), cos(data$pickup_latitude) * sin(data$dropoff_latitude) - sin(data$pickup_latitude) * cos(data$dropoff_latitude) * cos(data$dlon))

  #Some point of interest where people most use taxis in NYC
  jfk_coord_lat = (40.639722 * pi)/180
  jfk_coord_long = (-73.778889 * pi)/180
  ewr_coord_lat = (40.6925 * pi)/180
  ewr_coord_long = (-74.168611 * pi)/180
  lga_coord_lat = (40.77725 * pi)/180
  lga_coord_long = (-73.872611 * pi)/180
  liberty_statue_lat = (40.6892 * pi)/180
  liberty_statue_long = (-74.0445 * pi)/180
  nyc_lat = (40.7141667 * pi)/180
  nyc_long = (-74.0063889 * pi)/180
  
  #calculate distances radius for the POI
  data$JFK_dist = sphere_dist(data$pickup_latitude, data$pickup_longitude, jfk_coord_lat, jfk_coord_long) + sphere_dist(jfk_coord_lat, jfk_coord_long, data$dropoff_latitude, data$dropoff_longitude)
  data$EWR_dist = sphere_dist(data$pickup_latitude, data$pickup_longitude, ewr_coord_lat, ewr_coord_long) +  sphere_dist(ewr_coord_lat, ewr_coord_long, data$dropoff_latitude, data$dropoff_longitude)
  data$lga_dist = sphere_dist(data$pickup_latitude, data$pickup_longitude, lga_coord_lat, lga_coord_long) + sphere_dist(lga_coord_lat, lga_coord_long, data$dropoff_latitude, data$dropoff_longitude) 
  data$sol_dist = sphere_dist(data$pickup_latitude, data$pickup_longitude, liberty_statue_lat, liberty_statue_long) + sphere_dist(liberty_statue_lat, liberty_statue_long, data$dropoff_latitude, data$dropoff_longitude)
  data$nyc_dist = sphere_dist(data$pickup_latitude, data$pickup_longitude, nyc_lat, nyc_long) + sphere_dist(nyc_lat, nyc_long, data$dropoff_latitude, data$dropoff_longitude)
  
  #remove some columns
  drops <- c("key","pickup_datetime")
  data = data[, !(names(data) %in% drops)]
  print(dim(data))
  return (data)
  
}
```

```{r}
fdata = features_eng(data = fdata)
```

## Checking for outliers

** Several outliers are still present **

```{r}
fit_fy = lm(fare_amount ~ year, data = fdata)
plot(fare_amount ~ year, data = fdata, col = "grey", pch = 20, cex = 1.5,
     main = "Fare amount by year")
```

### Outlier Pickup locations 

** Here we are removing the outliers from the locations based on 1.5 * IQR **
```{r}
fdata = fdata[fdata$pickup_longitude < median(fdata$pickup_longitude) + 1.5 * IQR( fdata$pickup_longitude) &
              fdata$pickup_longitude > median(fdata$pickup_longitude - 1.5 * IQR(fdata$pickup_longitude)) &
              fdata$pickup_latitude < median(fdata$pickup_latitude) + 1.5 * IQR(fdata$pickup_latitude) &
              fdata$pickup_latitude > median(fdata$pickup_latitude) - 1.5 * IQR(fdata$pickup_latitude),]
  print(dim(fdata))
  # Outlier Dropoff locations
  fdata = fdata[fdata$dropoff_longitude < median(fdata$dropoff_longitude) + 1.5 * IQR(fdata$dropoff_longitude) &
              fdata$dropoff_longitude > median(fdata$dropoff_longitude - 1.5 * IQR(fdata$dropoff_longitude)) &
              fdata$dropoff_latitude < median(fdata$dropoff_latitude) + 1.5 * IQR(fdata$dropoff_latitude) &
              fdata$dropoff_latitude > median(fdata$dropoff_latitude) - 1.5 * IQR(fdata$dropoff_latitude),]
  print(dim(fdata))
  # Outlier Fare Amount
fdata = fdata[fdata$fare_amount < median(fdata$fare_amount) + 1.5 * IQR(fdata$fare_amount),]
```

## Looks like the outliers are pretty much handle
```{r}
fit_fy = lm(fare_amount ~ year, data = fdata)
plot(fare_amount ~ year, data = fdata, col = "grey", pch = 20, cex = 1.5,
     main = "Fare amount by year")
```

```{r}
plot(fare_amount ~ weekday, data = fdata,
     main   = "Fare Amount x weekday",
     col = "grey", pch = 20, cex = 1.5)
```

## Lets split the data for training and test

```{r}
inds = sample.int(nrow(fdata), size=nrow(fdata) * 0.9) 
train = fdata[inds,]
test = fdata[-inds,]
dim(train)
dim(test)
head(train)
```

## seeting the RMSE and LOORMSE functions

```{r}
calc_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

calc_rmse2 = function(model, data){
  sqrt(sum((data$fare_amount - predict(model, newdata=data))^2) / nrow(data))
}
```


## Multiple Linear Regression

** Let start by doing a simple baseline model and check the results **

```{r}
m_additive = lm(fare_amount ~ dropoff_longitude + dropoff_latitude + pickup_latitude + pickup_longitude + passenger_count + day + weekday + time + dlat + dlon + haversine + bearing, train)
```

** Looks like correlation is over control not finding any > 7 **

```{r}
vif(m_additive)
```

**Both RMSE and LOORMSE are close giving a difference between 1.75 and 1.76 dolars **

```{r}
calc_rmse(m_additive)
calc_rmse2(m_additive, test)
```

** Let's try to find better features from the additive model to try to get better results **

```{r}
n = length(resid(m_additive))
m_add_mod_back_bic = step(m_additive, direction = "both", k = log(n))
```

** the model with Step:  AIC=73136.14 was the best one for this amount of data, lets check the results**

```{r}
m_additive2 = lm(fare_amount ~ dropoff_longitude + dropoff_latitude + pickup_latitude + 
    pickup_longitude + passenger_count + weekday + time + haversine + 
    bearing, train)
```

** correlation still under control**

```{r}
vif(m_additive2)
```

** the model perform a little better but not much to be considered**
```{r}
calc_rmse(m_additive2)
calc_rmse2(m_additive2, test)
```

** lets check the statistics and the p-value for both additive models**

```{r}
summary(m_additive)
summary(m_additive2)
anova(m_additive2, m_additive)
```

** both models have the same p-value < 2.2e-16, and the comparision the model 1 perform a little better, but not to create a bigger difference **

## Second model with interations

```{r}
m_int = lm(fare_amount ~ .^2, train)
```

** we clear see that the results are better, but probably there is some overfit since we are interatcting all the features**

** the RMSE and LOORMSE  are clear better lowering the amount of errors in dollars by 1.48 - 1.50 **

```{r}
calc_rmse(m_int)
calc_rmse2(m_int, test)
```

```{r}
anova(m_additive2, m_int)
```

** comparing both the aditive and the interactive, the interactive is much better so we assume it as  the  better model **

```{r}
summary(m_int)
```


## Conclusion
###  The interaction between features will produce a better model, but is overfitting and not generalizing well. The data is to sparse and a Linear Regression  is not able to understand the data completely. 

### We added a different approach, xgboost_canivel2.Rmd, using XGboost, that clear achive much better results with unseeing  data.

### A web client can be found at the client/webapp.R, it runs local predicting the value using  the XGBoost model.
